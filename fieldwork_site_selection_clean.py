# -*- coding: utf-8 -*-
"""fieldwork_site_selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TSRkf_ymbfs0vHPiusyT0ojIxwV5Sf3y
"""

#Here we are importing the packages we need. 
import geopandas as gpd
import pandas as pd 
from geopandas.tools import sjoin
from shapely.geometry import LineString
from shapely.geometry import Point
from shapely.geometry import Polygon
from shapely.geometry import shape
import alphashape
from descartes import PolygonPatch
import time
import math
import scipy.stats as stats
import numpy as np
import os, sys
from pyproj import CRS, Transformer
import fiona

import statsmodels.api as sm
import statsmodels.formula.api as smf

import matplotlib.pyplot as plt
import matplotlib as mpl
from math import floor

from shapely.ops import unary_union

import warnings
warnings.filterwarnings('ignore')

from osgeo import ogr, gdal,osr

from ortools.linear_solver import pywraplp
import string

from shapely.geometry.multipolygon import MultiPolygon

def get_potential_sites(gdf,spacing): 

    bounds = gdf.bounds
    xmax = bounds['maxx']
    xmin= bounds['minx']
    ymax = bounds['maxy']
    ymin = bounds['miny']

    num_col = int((xmax - xmin) / spacing) 
    num_row = int((ymax - ymin) / spacing)

    Yi = np.linspace(ymin,ymax,num_row+1)
    Xi = np.linspace(xmin,xmax,num_col+1)
    Xi,Yi = np.meshgrid(Xi,Yi)

    points = [Point(x) for x in zip(Xi.flatten(),Yi.flatten())]
    points = gpd.GeoDataFrame(geometry=gpd.GeoSeries(points),crs='esri:102001')\
    .to_crs('EPSG:4326')

    points['lat'] = list(points['geometry'].y)
    points['lon'] = list(points['geometry'].x)


    return points


def join_shapefile(gdf,aerial_survey,col_name,fire=False,harvest=False,road=False): 
    if fire == False and harvest == False and road == False: 
        points_in_shp = sjoin(gdf, aerial_survey, how='left')
        points_in_shp['FOREST_DAM'] = points_in_shp['FOREST_DAM'].fillna(0)
        points_in_shp.loc[points_in_shp['FOREST_DAM'] != 0, 'FOREST_DAM'] = 1
        points_in_shp[col_name] = points_in_shp['FOREST_DAM']

    elif harvest == True and fire == False and road == False: 
        points_in_shp = sjoin(gdf, aerial_survey, how='left')
        points_in_shp['HarvestCon'] = points_in_shp['HarvestCon'].fillna(0)
        points_in_shp.loc[points_in_shp['HarvestCon'] != 0, 'HarvestCon'] = 1
        points_in_shp[col_name] = points_in_shp['HarvestCon']     

    elif road == True and fire == False and harvest == False: 
        points_in_shp = sjoin(gdf, aerial_survey, how='left')
        points_in_shp['LENGTH'] = points_in_shp['LENGTH'].fillna(0)
        points_in_shp.loc[points_in_shp['LENGTH'] != 0, 'LENGTH'] = 1
        points_in_shp[col_name] = points_in_shp['LENGTH'] 

    else:
        points_in_shp = sjoin(gdf, aerial_survey, how='left')
        points_in_shp['FIRE_ID'] = points_in_shp['FIRE_ID'].fillna(0)
        points_in_shp.loc[points_in_shp['FIRE_ID'] != 0, 'FIRE_ID'] = 1
        points_in_shp[col_name] = points_in_shp['FIRE_ID']
        df = points_in_shp.drop_duplicates(subset=['lat', 'lon'])
        points_in_shp = df
        #points_in_shp = df[~pd.DataFrame(np.sort(df[['lat','lon']].values,1)).duplicated()]

        

    array = np.array(points_in_shp[col_name])
    Xi = np.array(points_in_shp['lon'])
    Yi = np.array(points_in_shp['lat'])

    new_lon = []
    new_lat = []
    new_val = [] 
    for val,lon,lat in zip(array.flatten(),Xi.flatten(),Yi.flatten()):
        new_lon.append(lon)
        new_lat.append(lat)
        new_val.append(val)    

    points = [Point(x) for x in zip(Xi.flatten(),Yi.flatten())]

    print(len(new_val))
    gdf[col_name] = new_val

    return gdf

def get_dist_boundary(points,polygons,year): 
    new_gdf = gpd.GeoDataFrame(points,geometry=points['geometry'])
    
    proj_p = polygons.to_crs('esri:102001')
    buff = proj_p.buffer(10)
    #buff = unary_union([mp for mp in buff])
    diff = proj_p.symmetric_difference(buff)
    
    diff = unary_union([mp for mp in diff])
    
    plc = pd.DataFrame() 
    plc['hold'] = [1]

    polygons = gpd.GeoDataFrame(plc,geometry=[diff])
    
    points = points.to_crs('esri:102001')
    points_in_shp = sjoin(points, polygons, how='left')
    
    points_in_shp['in_boundary'+str(year)] = points_in_shp['hold'].fillna(0)
    
    points_in_shp.loc[points_in_shp['in_boundary'+str(year)] != 0, 'in_boundary'+str(year)] = 1
    
    list_insert = list(points_in_shp['in_boundary'+str(year)])
    
    new_gdf = new_gdf.loc[:,~new_gdf.columns.duplicated()]

    new_gdf['in_boundary'+str(year)] = list_insert
    
    return points_in_shp

def get_species(gdf,interpolated_surface,transform,\
                    size,srcds,col_name):
    '''This is a function to get the value inside the fire.
    We will use to calculate the mean, median, max value for a fire.
    
    Parameters
    ----------
        gdf : GeoPandas DataFrame
            gdf containing potential site points
        interpolated_surface : ndarray
            an array of values in the study area
        transform : list 
            list describing GeoTransform of raster 
        size : list 
            pixel dimensions
        srcds : GDAL object
            read in raster
            
    Returns
    ----------
        gdf 
            GeoPandas DataFrame with species values as column
    '''


    #Get information about the raster - what is pixel size? What is the origin?
    xOrigin = transform[0]
    yOrigin = transform[3]
    xMax = xOrigin + transform[1] * size[0]
    yMin = yOrigin + transform[5] * size[1]
    pixelWidth = transform[1]
    pixelHeight = -transform[5]  #-

    value = []
    lon = []
    lat = [] 
    
    # Iterate through the points within the fire 
    for idx,p in gdf.iterrows():
        #mx , my is the lon lat 
        mx,my=np.array(p['geometry'].coords.xy[0])[0], np.array(p['geometry'].coords.xy[1])[0]
        col = int((mx - xOrigin) / pixelWidth)
        row = int((yOrigin - my ) / pixelHeight) 
        # Calculate offset on the array 
        if row < interpolated_surface.shape[0] and row > 0 and col < interpolated_surface.shape[1] and col >0: 
            # Index the array where the point is and send to list to store it 
            value.append(interpolated_surface[row][col])
        else: 
            value.append(np.nan)

        lon.append(mx)
        lat.append(my)

    #Add values in list to the dataframe 
    gdf[col_name] = value

    return gdf

def get_dist_driving(G, gdf1,gdf2): 
    site_options = {} 
    for x,y in zip(gdf1['lon'],gdf1['lat']):
        df = pd.DataFrame() 
        dist_list = [] 
        lon_list = [] 
        lat_list = [] 
        for x2,y2 in zip(gdf2['lon'],gdf2['lat']): 
            if (x,y,) != (x2,y2): 

                # get the nearest network node to each point
                orig_node = ox.get_nearest_node(G, (y,x))
                dest_node = ox.get_nearest_node(G, (y2,x2))

                # how long is our route in meters?
                d = nx.shortest_path_length(G, orig_node, dest_node, weight='length')
                dist_list.append(d)
                lon_list.append(x2)
                lat_list.append(y2)

        df['dist'] = dist_list
        df['lon'] = lon_list
        df['lat'] = lat_list
        site_options[(x,y,)] = df

import math 
def get_dist_haversine(gdf1,gdf2): 
    site_options = {} 
    for x,y in zip(gdf1['lon'],gdf1['lat']):
        df = pd.DataFrame() 
        dist_list = [] 
        lon_list = [] 
        lat_list = [] 
        for x2,y2 in zip(gdf2['lon'],gdf2['lat']): 
            if (x,y,) != (x2,y2): 

                lon1, lat1, lon2, lat2 = map(math.radians, [x, y, x2, y2])

                # haversine formula 
                dlon = lon2 - lon1 
                dlat = lat2 - lat1 
                a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2
                c = 2 * math.asin(math.sqrt(a)) 
                r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.
                dist = c * r
                dist_list.append(dist)
                lon_list.append(x2)
                lat_list.append(y2)

        df['dist'] = dist_list
        df['lon'] = lon_list
        df['lat'] = lat_list
        site_options[(x,y,)] = df

    return site_options

def get_dist_haversine_polygon(gdf1,gdf2):
    pdf1 = gdf1.centroid
    pdf2 = gdf2.centroid
    site_options = {} 
    for x,y,id_num in zip(pdf1.x,pdf1.y,gdf1['id_num']):
        df = pd.DataFrame() 
        dist_list = [] 
        lon_list = [] 
        lat_list = []
        id_list = []
        for x2,y2,id_num2 in zip(pdf2.x,pdf2.y,gdf2['id_num']): 
            if (x,y,) != (x2,y2): 

                lon1, lat1, lon2, lat2 = map(math.radians, [x, y, x2, y2])

                # haversine formula 
                dlon = lon2 - lon1 
                dlat = lat2 - lat1 
                a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2
                c = 2 * math.asin(math.sqrt(a)) 
                r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.
                dist = c * r
                dist_list.append(dist)
                lon_list.append(x2)
                lat_list.append(y2)
                id_list.append(id_num2)

        df['polyid'] = id_list 
        df['dist'] = dist_list
        df['lon'] = lon_list
        df['lat'] = lat_list
        site_options[(x,y,)] = df

    return site_options


def optimize_for_site(site,dictionary_list):

    information = pd.DataFrame()
    store_pot_site = [] 
    for dictionary in dictionary_list:

        pot_site = dictionary[site].reset_index(drop=True)
        store_pot_site.append(pot_site)

    #print(store_pot_site)
    solver = pywraplp.Solver.CreateSolver('SCIP')

    var_list = []
    dist_list = [] 
    lon_list = [] 
    lat_list = [] 
    var_list_sites = {}
    id_list= [] 

    count = 0
    checkpoints = list(range(1,len(store_pot_site)+1))
    
    for list_ in store_pot_site:
        
        storage = [] 
        count+=1 
        count2 = 0 #count 2 is the site index 
        for inst in list_.iterrows(): 
            inst = list(inst)[1]
            var_list.append('x'+str(count)+'_'+str(count2))
            dist_list.append(inst['dist'])
            lon_list.append(inst['lon'])
            lat_list.append(inst['lat'])
            id_list.append(inst['polyid'])
            
            storage.append('x'+str(count)+'_'+str(count2))
            count2+=1 

        var_list_sites[count] = storage
    

    var_collect = {}


    merge_list = [j for i in var_list_sites.values() for j in i]
    
    for i in range(0, len(var_list)): 
        # solver.IntVar(0.0, 1, var_list[i]) --> tell computer that the variable can
        # only be 1 or 0. 
        var_collect[var_list[i]] = solver.IntVar(0.0, 1, var_list[i])

                
    information['variable'] = var_list #merge_list
    
    information['distance'] = dist_list 
    information['lat'] = lat_list
    information['lon'] = lon_list
    information['id_num'] = id_list 

    locals().update(var_collect)
    globals().update(var_collect)
    append_lists = []

    for var in var_list_sites.values(): 
        all_vars = [eval(i) for i in var]
        append_lists.append(all_vars)

    for list_of_con in append_lists:
        
        solver.Add(sum(list_of_con) == 1)

    #print('Number of constraints =', solver.NumConstraints()) 
    #print('Number of variables =', solver.NumVariables())

    #Tell computer that each component of the objective function is dist * variable
    concat_list = [j for i in append_lists for j in i]
    
    obj_collect = [i*j for i, j in zip(concat_list,dist_list)]
    #Tell the computer to maximize the sum of these volume*variable pairs 
    solver.Minimize(sum(obj_collect))

    #Solve 
    status = solver.Solve()

    #Get information from solver 
    decision = [] 
    if status == pywraplp.Solver.OPTIMAL:
        
        #print('Objective value =', solver.Objective().Value())
        for var in concat_list: 
          #Append to decision list the solution for the specific variable. 
          decision.append(abs(var.solution_value()))
    else:
        print('The problem does not have an optimal solution.')

    if len(decision) > 0: 
      #Append a row to the original table IF there is an optimal solution
      information['visit'] = decision 

      #Here's our table. 'cut' tells us whether to harvest or not. 
      #print(information[information['visit'] == 1])
    
    information = information[information['visit'] == 1]

    
    return float(solver.Objective().Value()),information

def concave_hull(points_df,alpha):
    points_df = points_df.to_crs('esri:102001')
    points = points_df['geometry']

    hull = alphashape.alphashape(points,alpha)
    try: 
        hull = gpd.GeoSeries([MultiPolygon([i]) for i in hull])
    except:
        hull = gpd.GeoSeries([MultiPolygon([hull])])
    sum_areas = sum([i.area / 10**6 for i in hull]) 
    
    
    polygon = gpd.GeoDataFrame(crs='esri:102001', geometry=hull).to_crs('EPSG:4326')
    
    fig, ax = plt.subplots()
    polygon.plot(ax=ax,facecolor='#81C784',edgecolor='#81C784',zorder=10,label='Estimate',linewidth=1,aspect=1)
    ax.set_xticks([])
    ax.set_yticks([])
    plt.show()


    return polygon


if __name__ == "__main__":

    
##    #Make the potential sites
##    sa = gpd.read_file('study_area/area_of_interest.shp').to_crs('esri:102001')
##    sites_df = get_potential_sites(sa,500)
##
##    #Join data to make a single table for analysis
##
##    fires = gpd.read_file('site_selection/fire_extract_2009_plus_fix_geom.shp')
##    fire_df = join_shapefile(sites_df,fires,'fire_detected',fire=True)
##
##    mortality12 = gpd.read_file('aerial_survey_maps/2009_ON_2022_02_10_CR.shp')
##    mortality12 = mortality12[mortality12['FOREST_DAM'] == 'Mortality']
##    ca12_df = join_shapefile(fire_df,mortality12,'ca12_detected',fire=False)
##    print(ca12_df[ca12_df['ca12_detected'] >= 1])
##
##    mortality8 = gpd.read_file('aerial_survey_maps/2012_ON_2022_02_10_CR.shp')
##    mortality8 = mortality8[mortality8['FOREST_DAM'] == 'Mortality']
##    ca8_df = join_shapefile(fire_df,mortality8,'ca8_detected',fire=False)
##    print(ca8_df[ca8_df['ca8_detected'] >= 1])
##
##    mortality7 = gpd.read_file('aerial_survey_maps/2014_ON_2022_02_10_CR.shp')
##    mortality7 = mortality7[mortality7['FOREST_DAM'] == 'Mortality']
##    ca7_df = join_shapefile(ca8_df,mortality7,'ca7_detected',fire=False)
##
##    mortality5 = gpd.read_file('aerial_survey_maps/2016_ON_2022_02_10_CR.shp')
##    mortality5 = mortality5[mortality5['FOREST_DAM'] == 'Mortality']
##    ca5_df = join_shapefile(ca7_df,mortality5,'ca5_detected',fire=False)
##
##    mortality2 = gpd.read_file('aerial_survey_maps/2019_ON_2022_02_10_CR.shp')
##    mortality2 = mortality2[mortality2['FOREST_DAM'] == 'Mortality']
##    ca2_df = join_shapefile(ca5_df,mortality2,'ca2_detected',fire=False)
##
##    mortality1 = gpd.read_file('aerial_survey_maps/2020_ON_2022_02_10_CR.shp')
##    mortality1 = mortality1[mortality1['FOREST_DAM'] == 'Mortality']
##    ca1_df = join_shapefile(ca2_df,mortality1,'ca1_detected',fire=False)
##
##    ca7_df = ca1_df
##
####    mortality12 = gpd.read_file('aerial_survey_maps/2009_ON_2022_02_10_CR.shp')
####    mortality12 = mortality12[mortality12['FOREST_DAM'] == 'Mortality']
####    if 'index_right' in list(ca7_df): 
####      ca7_df = ca7_df.drop('index_right', 1)
####    if 'hold' in list(ca7_df): 
####      ca7_df = ca7_df.drop('hold', 1)
####    ca7_df = get_dist_boundary(ca7_df,mortality12,2009)
####
####    mortality9 = gpd.read_file('aerial_survey_maps/2012_ON_2022_02_10_CR.shp')
####    mortality9 = mortality9[mortality9['FOREST_DAM'] == 'Mortality']
####    if 'index_right' in list(ca7_df): 
####      ca7_df = ca7_df.drop('index_right', 1)
####    if 'hold' in list(ca7_df): 
####      ca7_df = ca7_df.drop('hold', 1)
####    ca7_df = get_dist_boundary(ca7_df,mortality12,2012)
####    
####    mortality7 = gpd.read_file('aerial_survey_maps/2014_ON_2022_02_10_CR.shp')
####    mortality7 = mortality7[mortality7['FOREST_DAM'] == 'Mortality']
####    if 'index_right' in list(ca7_df): 
####      ca7_df = ca7_df.drop('index_right', 1)
####    if 'hold' in list(ca7_df): 
####      ca7_df = ca7_df.drop('hold', 1)
####    ca7_df = get_dist_boundary(ca7_df,mortality7,2014)
####
####    mortality5 = gpd.read_file('aerial_survey_maps/2016_ON_2022_02_10_CR.shp')
####    mortality5 = mortality5[mortality5['FOREST_DAM'] == 'Mortality']
####    if 'index_right' in list(ca7_df): 
####        ca7_df = ca7_df.drop('index_right', 1)
####    if 'hold' in list(ca7_df): 
####      ca7_df = ca7_df.drop('hold', 1)
####    ca7_df = get_dist_boundary(ca7_df,mortality5,2016)
####    
####
####    mortality2 = gpd.read_file('aerial_survey_maps/2019_ON_2022_02_10_CR.shp')
####    mortality2 = mortality2[mortality2['FOREST_DAM'] == 'Mortality']
####    if 'index_right' in list(ca7_df): 
####        ca7_df = ca7_df.drop('index_right', 1)
####    if 'hold' in list(ca7_df): 
####      ca7_df = ca7_df.drop('hold', 1)
####    ca7_df = get_dist_boundary(ca7_df,mortality2,2019)
####
####    mortality1 = gpd.read_file('aerial_survey_maps/2020_ON_2022_02_10_CR.shp')
####    mortality1 = mortality1[mortality1['FOREST_DAM'] == 'Mortality']
####    if 'index_right' in list(ca7_df): 
####        ca7_df = ca7_df.drop('index_right', 1)
####    if 'hold' in list(ca7_df): 
####      ca7_df = ca7_df.drop('hold', 1)
####    ca7_df = get_dist_boundary(ca7_df,mortality1,2020)
####    if 'index_right' in list(ca7_df): 
####        ca7_df = ca7_df.drop('index_right', 1)
####    if 'hold' in list(ca7_df): 
####      ca7_df = ca7_df.drop('hold', 1)
##
##    #Re-convert to geographic system  
##    ca7_df = ca7_df.to_crs('EPSG:4326')
##
##    # Open selected raster using gdal 
##    src_ds = gdal.Open('site_selection/fri_age.tif')
##    # Get band 1 
##    rb1=src_ds.GetRasterBand(1)
##    transform=src_ds.GetGeoTransform()
##    cols = src_ds.RasterXSize
##    rows = src_ds.RasterYSize
##
##    # Convert raster to array 
##    data = rb1.ReadAsArray(0, 0, cols, rows)
##    df_age = get_species(ca7_df,data,transform,(cols,rows,),src_ds,'age')
##
##
##    # Open selected raster using gdal 
##    src_ds = gdal.Open('site_selection/fri_bf.tif')
##    # Get band 1 
##    rb1=src_ds.GetRasterBand(1)
##    transform=src_ds.GetGeoTransform()
##    cols = src_ds.RasterXSize
##    rows = src_ds.RasterYSize
##
##    # Convert raster to array 
##    data = rb1.ReadAsArray(0, 0, cols, rows)
##    df_bf = get_species(df_age,data,transform,(cols,rows,),src_ds,'Bf')
##
##    # Open selected raster using gdal 
##    src_ds = gdal.Open('site_selection/fri_sw.tif')
##    # Get band 1 
##    rb1=src_ds.GetRasterBand(1)
##    transform=src_ds.GetGeoTransform()
##    cols = src_ds.RasterXSize
##    rows = src_ds.RasterYSize
##
##    # Convert raster to array 
##    data = rb1.ReadAsArray(0, 0, cols, rows)
##    df_sw = get_species(df_bf,data,transform,(cols,rows,),src_ds,'Sw')
##
##    # Open selected raster using gdal 
##    src_ds = gdal.Open('site_selection/fri_sb.tif')
##    # Get band 1 
##    rb1=src_ds.GetRasterBand(1)
##    transform=src_ds.GetGeoTransform()
##    cols = src_ds.RasterXSize
##    rows = src_ds.RasterYSize
##
##    # Convert raster to array 
##    data = rb1.ReadAsArray(0, 0, cols, rows)
##    df_sb = get_species(df_sw,data,transform,(cols,rows,),src_ds,'Sb')
##
##    #Get rid of NA
##    df_sb = df_sb.dropna(how='any')
##
##    harvest = gpd.read_file('site_selection/harvest_buffer_2009_plus.shp')
##
##    harvest_df = join_shapefile(df_sb,harvest,'harvest_detected',harvest=True)
##
##    road = gpd.read_file('site_selection/roads_eastern_ON_buffer.shp')
##
##    road_df = join_shapefile(harvest_df,road,'road_detected',road=True)
##
##    #Finally, the combined table
##    df_all = road_df.dropna(how='any')
##    df_all.to_csv('table_data.txt',sep=',')

    df_all = pd.read_csv('table_data.txt',sep=',')


    #Filter out harsh constraints 
    df_all = df_all[df_all['fire_detected'] == 0]
    df_all = df_all[df_all['harvest_detected'] == 0]

    df_all['combo_species'] = df_all['Bf'] + df_all['Sw'] #+df_all['Sb']
    df_all = df_all[df_all['combo_species'] >= 30]
    df_all = df_all[df_all['age'] >= 40]


    site1 = df_all[df_all['ca1_detected'] == 1]
    site1 = site1[site1['ca2_detected'] == 0]
    site1 = site1[site1['ca5_detected'] == 0]
    site1 = site1[site1['ca7_detected'] == 0]
    site1 = site1[site1['ca8_detected'] == 0]

    #site1 = site1[site1['in_boundary2020'] == 0]
    site1 = site1[site1['road_detected'] == 1]
    print(site1)


    site2 =  df_all[df_all['ca2_detected'] == 1]
    site2 = site2[site2['ca5_detected'] == 0]
    site2 = site2[site2['ca7_detected'] == 0]
    site2 = site2[site2['ca8_detected'] == 0]
    site2 = site2[site2['ca1_detected'] == 0]
    site2 = site2[site2['ca12_detected'] == 0]
    #site2 = site2[site2['in_boundary2016'] == 0]
    site2 = site2[site2['road_detected'] == 1]
    print(site2)


    site5 =  df_all[df_all['ca5_detected'] == 1]
    site5 = site5[site5['ca7_detected'] == 0]
    site5 = site5[site5['ca8_detected'] == 0]
    site5 = site5[site5['ca1_detected'] == 0]
    site5 = site5[site5['ca2_detected'] == 0]
    site5 = site5[site5['ca12_detected'] == 0]
    #site5 = site5[site5['in_boundary2016'] == 0]
    site5 = site5[site5['road_detected'] == 1]
    print(site5)
    
    site7 =  df_all[df_all['ca7_detected'] == 1]
    #site7 = site7[site7['in_boundary2014'] == 0]
    site7 = site7[site7['ca8_detected'] == 0]
    site7 = site7[site7['ca1_detected'] == 0]
    site7 = site7[site7['ca2_detected'] == 0]
    site7 = site7[site7['ca5_detected'] == 0]
    site7 = site7[site7['ca12_detected'] == 0]
    site7 = site7[site7['road_detected'] == 1]

    site8 =  df_all[df_all['ca8_detected'] == 1]
    site8 =  site8[site8['ca1_detected'] == 0]
    site8 =  site8[site8['ca2_detected'] == 0]
    site8 =  site8[site8['ca5_detected'] == 0]
    site8 =  site8[site8['ca7_detected'] == 0]
    site8 =  site8[site8['ca12_detected'] == 0]
    site8 = site8[site8['road_detected'] == 1]
    #site8 = site8[site8['in_boundary2009'] == 0]

    site9 =  df_all[df_all['ca12_detected'] == 1]
    site9 =  site9[site9['ca1_detected'] == 0]
    site9 =  site9[site9['ca2_detected'] == 0]
    site9 =  site9[site9['ca5_detected'] == 0]
    site9 =  site9[site9['ca7_detected'] == 0]
    site9 =  site9[site9['ca8_detected'] == 0]
    site9 = site9[site9['road_detected'] == 1]

    print(site1)
    print(site2)
    print(site5)
    print(site8)
    print(site9)

    polygon_list = []

    for df in [site1,site2,site5,site7,site8,site9]:

        gdf = gpd.GeoDataFrame(df,geometry=gpd.points_from_xy(df['lon'],df['lat']),crs='EPSG:4326')
        
        try:
            
            site_poly = concave_hull(gdf,0.0005)
        except:
            proj = gdf.to_crs('esri:102001')
            poly_buff = proj['geometry'].buffer(500)
            site_poly = gpd.GeoDataFrame(df,geometry=poly_buff,crs='esri:102001').reset_index(drop=True)
        site_poly['id_num'] = list(range(0,len(site_poly)))

        polygon_list.append(gpd.GeoDataFrame(site_poly,geometry=site_poly['geometry']).to_crs('epsg:4326'))

    site1 = polygon_list[0]
    site2 = polygon_list[1]
    site5 = polygon_list[2]
    site7 = polygon_list[3]
    site8 = polygon_list[4]
    site9 = polygon_list[5]
    
    dd1 = get_dist_haversine_polygon(site7,site1) #There are the least site 7's so we optimize distance to them
    dd2 = get_dist_haversine_polygon(site7,site2)
    dd5 = get_dist_haversine_polygon(site7,site5)
    dd8 = get_dist_haversine_polygon(site7,site8)
    dd9 = get_dist_haversine_polygon(site7,site9)

    
    #Optimize 
    op_list = [] 
    mdist_list = [] 
    site7_loc = [] 
    for row in site7[['geometry','id_num']].iterrows():
        site = row
        print(list(row)[1])
        site = list(site)[1][0]
        site = gpd.GeoDataFrame(geometry=[site]) #Site[0]
        site = site.centroid
        loc = (float(site.x),float(site.y),)
        
        obj_func, info = optimize_for_site(loc,[dd1,dd2,dd5,dd8,dd9])
        op_list.append(info)
        mdist_list.append(obj_func)
        site7_loc.append([float(site.x),float(site.y),list(row)[0]])
    index_min = mdist_list.index(min(mdist_list))

    print('Selected Sites')
    print(op_list[index_min])
    print('Selected Site 7 Location')
    print(site7_loc[index_min])
    print('Minimum Haversine Dist Possible Bt Sites (km)')
    print(mdist_list[index_min])

    df_sites = op_list[index_min]

    df_sites = df_sites.append(pd.DataFrame([['x0', 0,site7_loc[index_min][1],\
                                   site7_loc[index_min][0],site7_loc[index_min][2],1.0]],columns=df_sites.columns))

    df_sites['years_act'] = [1,2,5,9,12,7]
    df_sites = df_sites.sort_values('years_act')
    print(df_sites)

    #Now look up the geometry for each site
    geo_list = []
    count = 0
    for id_num in list(df_sites['id_num']):
        df = polygon_list[count]
        geo_list.append(df['geometry'][id_num])
        count+=1

    new_list = [] 
    for x in geo_list:
        count = 0 
        updated_geo_list = []
        for y in geo_list:
            if y != x:
                if count == 0: 
                    diff = x.difference(y)
                    updated_geo_list.append(diff)
                elif count == 1:
                    diff = x.difference(updated_geo_list[0])
                    updated_geo_list.append(diff)
                else:
                    diff = x.difference(updated_geo_list[-1])
                    updated_geo_list.append(diff)
                count +=1 
                    
        new_list.append(updated_geo_list[-1])
            
        

    df_sites['geometry'] = new_list

    gdf_sites = gpd.GeoDataFrame(df_sites,geometry=\
                             df_sites['geometry'])

    #Now you need to make sure they do not overlap!!!!!!!
    gdf_sites.to_file(driver = 'ESRI Shapefile',filename='polygon_group1')
    gdf_sites.to_file(driver='GeoJSON',filename='polygon_group1_gjson')

    first_site7 = site7_loc[index_min]

    #Optimize 2
    dd1_upd = {} 
    for key, val in dd1.items():
        site1 = df_sites[df_sites['years_act'] == 1]
        val = val[~((val['lat'] == float(site1['lat'])) & (val['lon'] == float(site1['lon'])))]
        dd1_upd[key] = val

    dd2_upd = {} 
    for key, val in dd2.items():
        site2 = df_sites[df_sites['years_act'] == 2]
        val = val[~((val['lat'] == float(site2['lat'])) & (val['lon'] == float(site2['lon'])))]
        dd2_upd[key] = val

    dd5_upd = {} 
    for key, val in dd5.items():
        site5 = df_sites[df_sites['years_act'] == 5]
        val = val[~((val['lat'] == float(site5['lat'])) & (val['lon'] == float(site5['lon'])))]
        dd5_upd[key] = val

    dd8_upd = {} 
    for key, val in dd8.items():
        site8 = df_sites[df_sites['years_act'] == 9]
        val = val[~((val['lat'] == float(site8['lat'])) & (val['lon'] == float(site8['lon'])))]
        dd8_upd[key] = val

    dd9_upd = {} 
    for key, val in dd9.items():
        site9 = df_sites[df_sites['years_act'] == 12]
        val = val[~((val['lat'] == float(site9['lat'])) & (val['lon'] == float(site9['lon'])))]
        dd9_upd[key] = val

        
        
    op_list = [] 
    mdist_list = [] 
    site7_loc = [] 
    for site in site7.iterrows(): 
        site = list(site)[1]
        loc = (site['lon'],site['lat'],)
        if loc != first_site7: 
            obj_func, info = optimize_for_site(loc,[dd1_upd,dd2_upd,dd5_upd,dd8_upd,dd9_upd])
            op_list.append(info)
            mdist_list.append(obj_func)
            site7_loc.append(loc)
    index_min = mdist_list.index(min(mdist_list))


    df_sites2 = op_list[index_min]

    df_sites2 = df_sites2.append(pd.DataFrame([['x0', 0,site7_loc[index_min][1],\
                                   site7_loc[index_min][0],1.0]],columns=df_sites2.columns))

    df_sites2['years_act'] = [1,2,5,9,12,7]

    print(df_sites2)
    gdf_sites2 = gpd.GeoDataFrame(df_sites2,geometry=\
                             gpd.points_from_xy(df_sites2['lon'],df_sites2['lat']))
    gdf_sites2.to_file(driver = 'ESRI Shapefile',filename='sites2')
    gdf_sites2.to_file(driver='GeoJSON',filename='gsites2')

    second_site7 = site7_loc[index_min]

    #Optimize 3

  #Optimize 3
    dd1_upd2 = {} 
    for key, val in dd1_upd.items():
        site1 = df_sites2[df_sites2['years_act'] == 1]
        site0 = df_sites[df_sites['years_act'] == 1]
        val = val[~((val['lat'] == float(site1['lat'])) & (val['lon'] == float(site1['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        dd1_upd2[key] = val

    dd2_upd2 = {} 
    for key, val in dd2_upd.items():
        site2 = df_sites2[df_sites2['years_act'] == 2]
        site0 = df_sites[df_sites['years_act'] == 2]
        val = val[~((val['lat'] == float(site2['lat'])) & (val['lon'] == float(site2['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        dd2_upd2[key] = val

    dd5_upd2 = {} 
    for key, val in dd5_upd.items():
        site5 = df_sites2[df_sites2['years_act'] == 5]
        site0 = df_sites[df_sites['years_act'] == 5]
        val = val[~((val['lat'] == float(site5['lat'])) & (val['lon'] == float(site5['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        dd5_upd2[key] = val

    dd8_upd2 = {} 
    for key, val in dd8_upd.items():
        site8 = df_sites2[df_sites2['years_act'] == 9]
        site0 = df_sites[df_sites['years_act'] == 9]
        val = val[~((val['lat'] == float(site8['lat'])) & (val['lon'] == float(site8['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        dd8_upd2[key] = val

    dd9_upd2 = {} 
    for key, val in dd9_upd.items():
        site9 = df_sites2[df_sites2['years_act'] == 12]
        site0 = df_sites[df_sites['years_act'] == 12]
        val = val[~((val['lat'] == float(site9['lat'])) & (val['lon'] == float(site9['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        dd9_upd2[key] = val

        
    op_list = [] 
    mdist_list = [] 
    site7_loc = [] 
    for site in site7.iterrows(): 
        site = list(site)[1]
        loc = (site['lon'],site['lat'],)
        if loc != first_site7 and loc != second_site7: 
            obj_func, info = optimize_for_site(loc,[dd1_upd2,dd2_upd2,dd5_upd2,dd8_upd2,dd9_upd2])
            op_list.append(info)
            mdist_list.append(obj_func)
            site7_loc.append(loc)
    index_min = mdist_list.index(min(mdist_list))

    df_sites3 = op_list[index_min]

    df_sites3 = df_sites3.append(pd.DataFrame([['x0', 0,site7_loc[index_min][1],\
                                   site7_loc[index_min][0],1.0]],columns=df_sites3.columns))

    df_sites3['years_act'] = [1,2,5,9,12,7]

    print(df_sites3)

    gdf_sites3 = gpd.GeoDataFrame(df_sites3,geometry=\
                             gpd.points_from_xy(df_sites3['lon'],df_sites3['lat']))
    gdf_sites3.to_file(driver = 'ESRI Shapefile',filename='sites3')
    gdf_sites3.to_file(driver='GeoJSON',filename='gsites3')

    third_site7 = site7_loc[index_min]
    #Optimize 4
    dd1_upd3 = {} 
    for key, val in dd1_upd2.items():
        site3 = df_sites3[df_sites3['years_act'] == 1]
        site1 = df_sites2[df_sites2['years_act'] == 1]
        site0 = df_sites[df_sites['years_act'] == 1]
        val = val[~((val['lat'] == float(site1['lat'])) & (val['lon'] == float(site1['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        dd1_upd3[key] = val

    dd2_upd3 = {} 
    for key, val in dd2_upd2.items():
        site2 = df_sites2[df_sites2['years_act'] == 2]
        site3 = df_sites3[df_sites3['years_act'] == 2]
        site0 = df_sites[df_sites['years_act'] == 2]
        val = val[~((val['lat'] == float(site2['lat'])) & (val['lon'] == float(site2['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        dd2_upd3[key] = val

    dd5_upd3 = {} 
    for key, val in dd5_upd2.items():
        site5 = df_sites2[df_sites2['years_act'] == 5]
        site3 = df_sites3[df_sites3['years_act'] == 5]
        site0 = df_sites[df_sites['years_act'] == 5]
        val = val[~((val['lat'] == float(site5['lat'])) & (val['lon'] == float(site5['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        dd5_upd3[key] = val

    dd8_upd3 = {} 
    for key, val in dd8_upd2.items():
        site8 = df_sites2[df_sites2['years_act'] == 9]
        site0 = df_sites[df_sites['years_act'] == 9]
        site3 = df_sites3[df_sites3['years_act'] == 9]
        val = val[~((val['lat'] == float(site8['lat'])) & (val['lon'] == float(site8['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        dd8_upd3[key] = val

    dd9_upd3 = {} 
    for key, val in dd9_upd2.items():
        site9 = df_sites2[df_sites2['years_act'] == 12]
        site0 = df_sites[df_sites['years_act'] == 12]
        site3 = df_sites3[df_sites3['years_act'] == 12]
        val = val[~((val['lat'] == float(site9['lat'])) & (val['lon'] == float(site9['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        dd9_upd3[key] = val

        
    op_list = [] 
    mdist_list = [] 
    site7_loc = [] 
    for site in site7.iterrows(): 
        site = list(site)[1]
        loc = (site['lon'],site['lat'],)
        if loc != first_site7 and loc != second_site7 and loc != third_site7:
            obj_func, info = optimize_for_site(loc,[dd1_upd3,dd2_upd3,dd5_upd3,dd8_upd3,dd9_upd3])
            op_list.append(info)
            mdist_list.append(obj_func)
            site7_loc.append(loc)
    index_min = mdist_list.index(min(mdist_list))

    df_sites4 = op_list[index_min]

    df_sites4 = df_sites4.append(pd.DataFrame([['x0', 0,site7_loc[index_min][1],\
                                   site7_loc[index_min][0],1.0]],columns=df_sites4.columns))

    df_sites4['years_act'] = [1,2,5,9,12,7]

    print(df_sites4)

    gdf_sites4 = gpd.GeoDataFrame(df_sites4,geometry=\
                             gpd.points_from_xy(df_sites4['lon'],df_sites4['lat']))
    gdf_sites4.to_file(driver = 'ESRI Shapefile',filename='sites4')
    gdf_sites4.to_file(driver='GeoJSON',filename='gsites4')

    fourth_site7 = site7_loc[index_min]
    #Optimize 5
    dd1_upd4 = {} 
    for key, val in dd1_upd3.items():
        site4 = df_sites4[df_sites4['years_act'] == 1]
        site3 = df_sites3[df_sites3['years_act'] == 1]
        site1 = df_sites2[df_sites2['years_act'] == 1]
        site0 = df_sites[df_sites['years_act'] == 1]
        val = val[~((val['lat'] == float(site1['lat'])) & (val['lon'] == float(site1['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        val = val[~((val['lat'] == float(site4['lat'])) & (val['lon'] == float(site4['lon'])))]
        dd1_upd4[key] = val

    dd2_upd4 = {} 
    for key, val in dd2_upd3.items():
        site4 = df_sites4[df_sites4['years_act'] == 2]
        site2 = df_sites2[df_sites2['years_act'] == 2]
        site3 = df_sites3[df_sites3['years_act'] == 2]
        site0 = df_sites[df_sites['years_act'] == 2]
        val = val[~((val['lat'] == float(site2['lat'])) & (val['lon'] == float(site2['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        val = val[~((val['lat'] == float(site4['lat'])) & (val['lon'] == float(site4['lon'])))]
        
        dd2_upd4[key] = val

    dd5_upd4 = {} 
    for key, val in dd5_upd3.items():
        site4 = df_sites4[df_sites4['years_act'] == 5]
        site5 = df_sites2[df_sites2['years_act'] == 5]
        site3 = df_sites3[df_sites3['years_act'] == 5]
        site0 = df_sites[df_sites['years_act'] == 5]
        val = val[~((val['lat'] == float(site5['lat'])) & (val['lon'] == float(site5['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        val = val[~((val['lat'] == float(site4['lat'])) & (val['lon'] == float(site4['lon'])))]
        dd5_upd4[key] = val

    dd8_upd4 = {} 
    for key, val in dd8_upd3.items():
        site4 = df_sites4[df_sites4['years_act'] == 9]
        site8 = df_sites2[df_sites2['years_act'] == 9]
        site0 = df_sites[df_sites['years_act'] == 9]
        site3 = df_sites3[df_sites3['years_act'] == 9]
        val = val[~((val['lat'] == float(site8['lat'])) & (val['lon'] == float(site8['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        val = val[~((val['lat'] == float(site4['lat'])) & (val['lon'] == float(site4['lon'])))]
        dd8_upd4[key] = val

    dd9_upd4 = {} 
    for key, val in dd9_upd3.items():
        site4 = df_sites4[df_sites4['years_act'] == 12]
        site9 = df_sites2[df_sites2['years_act'] == 12]
        site0 = df_sites[df_sites['years_act'] == 12]
        site3 = df_sites3[df_sites3['years_act'] == 12]
        val = val[~((val['lat'] == float(site9['lat'])) & (val['lon'] == float(site9['lon'])))]
        val = val[~((val['lat'] == float(site0['lat'])) & (val['lon'] == float(site0['lon'])))]
        val = val[~((val['lat'] == float(site3['lat'])) & (val['lon'] == float(site3['lon'])))]
        val = val[~((val['lat'] == float(site4['lat'])) & (val['lon'] == float(site4['lon'])))]
        dd9_upd4[key] = val

        
    op_list = [] 
    mdist_list = [] 
    site7_loc = [] 
    for site in site7.iterrows(): 
        site = list(site)[1]
        loc = (site['lon'],site['lat'],)
        if loc != first_site7 and loc != second_site7 and loc != third_site7 and loc != fourth_site7:
            obj_func, info = optimize_for_site(loc,[dd1_upd3,dd2_upd3,dd5_upd3,dd8_upd3,dd9_upd3])
            op_list.append(info)
            mdist_list.append(obj_func)
            site7_loc.append(loc)
    index_min = mdist_list.index(min(mdist_list))

    df_sites5 = op_list[index_min]

    df_sites5 = df_sites5.append(pd.DataFrame([['x0', 0,site7_loc[index_min][1],\
                                   site7_loc[index_min][0],1.0]],columns=df_sites5.columns))

    df_sites5['years_act'] = [1,2,5,9,12,7]

    print(df_sites5)

    gdf_sites5 = gpd.GeoDataFrame(df_sites5,geometry=\
                             gpd.points_from_xy(df_sites5['lon'],df_sites5['lat']))
    gdf_sites5.to_file(driver = 'ESRI Shapefile',filename='sites5')
    gdf_sites5.to_file(driver='GeoJSON',filename='gsites5')


